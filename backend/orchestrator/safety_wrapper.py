"""
Safety Wrapper - Enforces medical safety boundaries from SAFETY_AND_SCOPE.md

This is the Safety & Guardrails Agent that wraps ALL AI outputs to ensure:
- Medical disclaimers are injected
- Prohibited language is blocked
- Confidence thresholds are enforced
- Red flags trigger escalations
- Hallucinations are detected

CRITICAL: All agent responses MUST pass through this wrapper before returning to user.
"""

from typing import Dict, List, Any, Optional
from orchestrator.base import AgentResponse, ConfidenceLevel
from datetime import datetime
import re
import logging

logger = logging.getLogger(__name__)


class SafetyViolation(Exception):
    """Raised when safety boundaries are violated"""
    pass


class SafetyWrapper:
    """
    Applies safety guardrails and medical disclaimers to all AI outputs.
    Implements rules from SAFETY_AND_SCOPE.md.
    """

    def __init__(self):
        # Prohibited language patterns (SAFETY_AND_SCOPE.md Â§3.2)
        self.prohibited_patterns = [
            r"\byou have\b",
            r"\byou definitely have\b",
            r"\bthis is definitely\b",
            r"\byou need to take\b",
            r"\bi prescribe\b",
            r"\bi recommend you take\b",
            r"\bstop taking your medication\b",
            r"\bdon't see a doctor\b",
            r"\bno need to see a doctor\b",
            r"\bdon't worry about\b",
            r"\bit's certainly\b",
            r"\bi'm certain\b",
            r"\b100% sure\b",
            r"\bdefinitive diagnosis\b",
        ]

        # Red flag keywords for emergency escalation (SAFETY_AND_SCOPE.md Â§5.1)
        self.red_flag_keywords = {
            "cardiac": [
                "chest pain", "heart attack", "myocardial infarction",
                "crushing chest pain", "radiating pain", "left arm pain",
                "cardiac arrest", "angina"
            ],
            "neurological": [
                "stroke", "facial droop", "slurred speech", "weakness one side",
                "sudden severe headache", "worst headache", "loss of consciousness",
                "seizure", "unresponsive", "confused"
            ],
            "respiratory": [
                "can't breathe", "difficulty breathing", "severe shortness of breath",
                "choking", "cyanosis", "blue lips", "stridor"
            ],
            "hemorrhage": [
                "severe bleeding", "hemorrhage", "blood loss", "vomiting blood",
                "coughing blood", "rectal bleeding severe"
            ],
            "psychiatric": [
                "suicide", "kill myself", "end my life", "self harm",
                "suicidal ideation"
            ]
        }

        # Medical disclaimers (SAFETY_AND_SCOPE.md Â§4)
        self.disclaimers = {
            "general": """âš ï¸ CLINICAL DECISION SUPPORT NOTICE
This information is generated by an AI system and is intended to support,
not replace, the relationship between patient and clinician. All suggestions
require review by a licensed healthcare professional. This is not a diagnosis.""",

            "diagnostic": """âš ï¸ DIFFERENTIAL DIAGNOSIS SUPPORT
The following represents possible conditions based on reported symptoms.
This is NOT a definitive diagnosis. Many conditions share similar presentations.
A thorough clinical examination, history, and appropriate testing are required
for accurate diagnosis. Consult a healthcare provider for evaluation.""",

            "image_analysis": """âš ï¸ IMAGE ANALYSIS SUPPORT
This AI-generated finding description is for clinician review only.
It does NOT constitute a radiological diagnosis. Images must be interpreted
by qualified radiologists or specialists. Critical findings require immediate
human expert review.""",

            "medication": """âš ï¸ MEDICATION INFORMATION
This information is educational only and does NOT constitute a prescription
or dosage recommendation. Drug interactions and contraindications shown are
based on database queries and may not be exhaustive. Consult a pharmacist
or prescribing physician before making any medication changes.""",

            "emergency": """âš ï¸ EMERGENCY ASSESSMENT
This triage suggestion is based on symptom patterns and is NOT a substitute
for clinical judgment. When in doubt, seek immediate medical attention.
Call emergency services (local emergency number) if experiencing:
- Chest pain or difficulty breathing
- Severe bleeding or trauma
- Loss of consciousness or confusion
- Sudden severe headache or stroke symptoms""",

            "voice": """âš ï¸ VOICE TRANSCRIPTION NOTICE
This transcription may contain errors. Clinicians must review and verify
all transcribed content before incorporating into medical records.
Do not rely solely on AI transcription for critical clinical information."""
        }

        # Emergency escalation message
        self.emergency_alert = """ðŸš¨ EMERGENCY INDICATORS DETECTED ðŸš¨
This situation may require IMMEDIATE medical attention.
Recommended action: Seek emergency care NOW.
Call [LOCAL EMERGENCY NUMBER] or go to nearest emergency department."""

    def wrap_response(
        self,
        response: AgentResponse,
        agent_type: str = "general"
    ) -> Dict[str, Any]:
        """
        Apply safety guardrails and disclaimers to agent response.

        Args:
            response: Raw agent response
            agent_type: Type of agent (diagnostic, image_analysis, etc.)

        Returns:
            Wrapped response with safety measures applied

        Raises:
            SafetyViolation: If prohibited content detected
        """
        logger.info(f"Applying safety wrapper to {agent_type} agent response")

        # Step 1: Check for prohibited language
        self._check_prohibited_language(response)

        # Step 2: Enforce confidence thresholds
        self._enforce_confidence_threshold(response)

        # Step 3: Check for emergency red flags
        emergency_detected = self._check_red_flags(response)

        # Step 4: Build wrapped response
        wrapped = {
            "success": response.success,
            "agent": response.agent_name,
            "timestamp": response.timestamp.isoformat(),
            "confidence": {
                "score": round(response.confidence * 100),
                "level": response.get_confidence_level().value,
                "emoji": response.get_confidence_emoji()
            },
            "data": response.data,
        }

        # Step 5: Add reasoning if available
        if response.reasoning:
            wrapped["reasoning"] = response.reasoning

        # Step 6: Add emergency alert if needed
        if emergency_detected or response.requires_escalation:
            wrapped["emergency"] = True
            wrapped["emergency_alert"] = self.emergency_alert
            wrapped["red_flags"] = response.red_flags

        # Step 7: Inject appropriate disclaimer
        disclaimer_type = self._get_disclaimer_type(agent_type, emergency_detected)
        wrapped["disclaimer"] = self.disclaimers.get(disclaimer_type, self.disclaimers["general"])

        # Step 8: Add safety metadata
        wrapped["safety_check"] = {
            "prohibited_language": False,
            "confidence_acceptable": response.confidence >= response.get_confidence_level().value,
            "red_flags_detected": len(response.red_flags) > 0,
            "escalation_required": response.requires_escalation or emergency_detected
        }

        return wrapped

    def _check_prohibited_language(self, response: AgentResponse):
        """
        Check for prohibited definitive language in response.

        Raises:
            SafetyViolation: If prohibited language detected
        """
        # Check in main data dictionary
        data_str = str(response.data).lower()

        for pattern in self.prohibited_patterns:
            if re.search(pattern, data_str, re.IGNORECASE):
                logger.error(f"Prohibited language detected: {pattern}")
                raise SafetyViolation(
                    f"Response contains prohibited definitive language. "
                    f"AI must not make definitive medical claims. "
                    f"See SAFETY_AND_SCOPE.md Â§3.2"
                )

    def _enforce_confidence_threshold(self, response: AgentResponse):
        """
        Check if confidence meets minimum threshold.

        Modifies response.data to include low confidence warning if needed.
        """
        confidence_level = response.get_confidence_level()

        if confidence_level == ConfidenceLevel.VERY_LOW:
            # Add warning about very low confidence
            response.data["_confidence_warning"] = (
                "âš ï¸ VERY LOW CONFIDENCE: The system has very low confidence in this assessment. "
                "This information may not be reliable. Recommend clinical evaluation by a healthcare provider."
            )

        elif confidence_level == ConfidenceLevel.LOW:
            response.data["_confidence_warning"] = (
                "âš ï¸ LOW CONFIDENCE: This assessment has low confidence. "
                "Consider alternative explanations and seek professional evaluation."
            )

    def _check_red_flags(self, response: AgentResponse) -> bool:
        """
        Check response for emergency red flag keywords.

        Returns:
            True if emergency detected, False otherwise
        """
        if response.red_flags:
            # Agent already identified red flags
            return True

        # Check data content for emergency keywords
        data_str = str(response.data).lower()

        for category, keywords in self.red_flag_keywords.items():
            for keyword in keywords:
                if keyword in data_str:
                    logger.warning(f"Red flag detected: {keyword} (category: {category})")
                    response.red_flags.append(f"{category}: {keyword}")
                    return True

        return False

    def _get_disclaimer_type(self, agent_type: str, emergency: bool) -> str:
        """
        Determine which disclaimer to use based on agent type and emergency status.

        Args:
            agent_type: Type of agent (diagnostic, image_analysis, etc.)
            emergency: Whether emergency detected

        Returns:
            Disclaimer key
        """
        if emergency:
            return "emergency"

        disclaimer_map = {
            "diagnostic_support": "diagnostic",
            "diagnostic": "diagnostic",
            "image_analysis": "image_analysis",
            "image": "image_analysis",
            "drug_info": "medication",
            "medication": "medication",
            "voice": "voice",
            "triage": "emergency"
        }

        return disclaimer_map.get(agent_type, "general")

    def check_hallucination(self, response: AgentResponse) -> bool:
        """
        Detect potential hallucinations in response.

        Current implementation: Basic heuristics
        Future: Use dedicated hallucination detection model

        Returns:
            True if potential hallucination detected
        """
        # Heuristic 1: Check for nonsensical medical terms
        nonsensical_patterns = [
            r"\b[a-z]{20,}\b",  # Very long words (possible gibberish)
            r"\d{10,}",          # Long number sequences
            r"[!?]{3,}",         # Excessive punctuation
        ]

        data_str = str(response.data)
        for pattern in nonsensical_patterns:
            if re.search(pattern, data_str):
                logger.warning(f"Potential hallucination detected: matched pattern {pattern}")
                return True

        # Heuristic 2: Check for conflicting confidence
        # If confidence is very high (>0.9) but data is minimal, might be hallucination
        if response.confidence > 0.9 and len(data_str) < 50:
            logger.warning("Potential hallucination: High confidence with minimal content")
            return True

        return False

    def format_emergency_response(self, red_flags: List[str]) -> Dict[str, Any]:
        """
        Format special emergency response for critical cases.

        Args:
            red_flags: List of detected red flags

        Returns:
            Emergency response dict
        """
        return {
            "emergency": True,
            "urgency_level": "ðŸš¨ EMERGENCY",
            "warning": self.emergency_alert,
            "red_flags_detected": red_flags,
            "immediate_action": "CALL EMERGENCY SERVICES (911) IMMEDIATELY or go to nearest emergency department",
            "do_not_delay": "This situation may be life-threatening. Do not wait for an appointment.",
            "disclaimer": self.disclaimers["emergency"]
        }


# Global singleton instance
safety_wrapper = SafetyWrapper()
